= Assignment — Thermal Fin ROM (HPC), simplified (CSV/JSON only)
:navtitle: Assignment — Simplified (CSV/JSON)
:icons: font
:source-highlighter: highlight.js
:stem: latexmath
:page-tags: assignment, hpc, slurm, apptainer, docker, rom

== Objective (very brief)
Build a **reproducible pipeline** to learn a **surrogate (ROM)** for the **2D thermal fin**:
run an ensemble of cases on the cluster, aggregate results, train a model with **uncertainty**, and provide a small **inference CLI**.

== Problem (minimal spec)
Parameters latexmath:[\mu=(k_1,k_2,k_3,k_4,\mathrm{Bi})] with
latexmath:[k_i\in[0.1,10]], latexmath:[\mathrm{Bi}\in[0.01,1]].
Quantity of interest (QoI): latexmath:[$T_{\text{root}}(\mu)=\int_{\Gamma_{\text{root}}}u\,ds$].
Default is **2D** (fast enough for N≈200–300); **3D** optional for demo/multi-fidelity (small N).

== What you must deliver
* **End-to-end pipeline**: Docker image published (GHCR) → **Apptainer** on the cluster → **Slurm array** (generation) → aggregation → training → evaluation.
* **Surrogate with UQ**: baseline **Gaussian Process** provided. Per group you may choose **one primary method** (e.g., RF/XGB, SVR, MLP, PCE, RB).
* **Traceability**: minimal run logs; (optional) MLflow or a simple CSV log of hyper-parameters and metrics.
* **Inference CLI**: prints `mean` and `std` for a given parameter vector.
* **Short report (2–3 pages, in English)**: data, method, hyper-parameters, **RMSE/NLL**, **95% coverage**, timings (train/infer), limitations.
* **Slides + short oral defense (in English)**.

== Milestones (no calendar, just checkpoints)
. **Local MVP**: synthetic generator → CSV → GP → inference (all local).
. **Image**: build locally, tag → CI pushes to **GHCR**.
. **HPC**: Apptainer `.sif` + **Slurm array** (N cases) + CSV aggregation.
. **Training & selection**: runs, metrics, best model chosen.
. **Packaging**: inference CLI usable; final image tag + report/slides.

== Technologies (short focus)
* **Docker** (or Podman): simple Dockerfile + `.dockerignore`; tag `vX.Y.Z` → **GHCR**.
* **Apptainer**: `apptainer pull docker://ghcr.io/ORG/REPO:tag` → user-space execution on the cluster.
* **Slurm**: **job arrays** (`--array`) + dependencies `afterok` (for Gaia, adjust the `-p` partition, e.g., `barret`).
* **Data**: **JSON lines** for per-case outputs, **CSV** for the aggregated training table (no Parquet, no BP).
* **Code**: Python 3.11, scikit-learn, numpy/pandas, joblib (keep it simple).
* **Feel++** (recommended): replace the synthetic generator with a PDE solve computing latexmath:[$T_{\text{root}}$].
* **Optional**: MLflow (file backend) or your own CSV log for runs; DVC is not required.

== Repository layout (proposed)
----
repo/
  containers/
    Dockerfile
    .dockerignore
  .github/workflows/
    docker-build-push.yml
  hpc/
    generate_array.sbatch      # Slurm (array) → data/raw/case_*.json
    aggregate.sbatch           # merge → data/processed/train.csv
    train_array.sbatch         # hyper-param sweep (if used)
    evaluate.sbatch            # pick best run and record its path
  src/
    pipeline/
      generate/
        make_params.py         # LHS → data/params.jsonl
        generate_point.py      # synthetic → replace by Feel++ call
        run_many_local.py      # local execution without Slurm
      aggregate/merge_csv.py   # merge JSON shards → CSV
      train/fit_gp.py          # GP baseline (prints metrics, saves model)
      infer/cli.py             # CLI: mean/std for a JSON theta
  data/                        # raw/, processed/ (not in Git)
  results/                     # models, logs (not in Git)
  Makefile                     # handy targets (mvp, gen, train…)
  README.adoc
----

== Slurm starter (Gaia / adjust partition)
[source,bash]
----
#!/bin/bash
#SBATCH -J fin-gen
#SBATCH --array=0-299
#SBATCH -c 2
#SBATCH -t 00:20:00
#SBATCH -p barret           # <-- adjust to your Gaia partition
module load apptainer
IDX=${SLURM_ARRAY_TASK_ID}
srun apptainer run app.sif \
  python -m pipeline.generate.generate_point \
    --params data/params.jsonl \
    --index ${IDX} \
    --out data/raw/case_${IDX}.json
----
[NOTE]
====
Use `--dependency=afterok:<JOBID>` to chain `aggregate` → `train` → `evaluate`.
====

== Minimal commands (examples)

*Local MVP*:
[source,bash]
----
python -m pipeline.generate.make_params --n 60 --out data/params.jsonl
python -m pipeline.generate.run_many_local --params data/params.jsonl --outdir data/raw/
python -m pipeline.aggregate.merge_csv --raw data/raw --out data/processed/train.csv
python -m pipeline.train.fit_gp --train data/processed/train.csv --outdir results/runs/0
python -m pipeline.infer.cli --modeldir results/runs/0 --theta '{"k1":1,"k2":1,"k3":1,"k4":1,"Bi":0.1}'
----

*Image & CI (GHCR)*:
[source,bash]
----
docker build -f containers/Dockerfile -t ghcr.io/ORG/REPO:local .
git tag v0.1.0 && git push origin v0.1.0
----

*Cluster*:
[source,bash]
----
module load apptainer
apptainer pull --name app.sif docker://ghcr.io/ORG/REPO:v0.1.0
sbatch hpc/generate_array.sbatch
sbatch hpc/aggregate.sbatch
sbatch hpc/train_array.sbatch
sbatch hpc/evaluate.sbatch
----

== Evaluation (condensed rubric)
* **Engineering (20%)**: image, GHCR release, Apptainer, Slurm (array/deps).
* **Data pipeline (15%)**: ensemble produced, CSV aggregation OK.
* **Modeling (35%)**: stability, RMSE/NLL, **calibration** (95% coverage).
* **Repro (15%)**: clear commands/logs, seeds, simple run records (CSV or MLflow).
* **Packaging & report (15%)**: working CLI + 2–3 page report + slides & short defense (English).

== Practical tips
- Keep per-case JSON small (one line per case). Aggregate once into a single **CSV** for training.
- Set `OMP_NUM_THREADS=1` for CPU reproducibility.
- Default **2D** gives enough samples; keep **3D** as optional bonus.
- Always keep the **synthetic generator** as a fallback to avoid blocking on the PDE wrapper.
